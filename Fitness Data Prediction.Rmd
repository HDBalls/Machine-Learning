---
title: "Exercise prediction from generated training data using a suitable algorithm."
author: "Bola Lawal"
date: '2022-05-06'
output:
  html_document:
    df_print: paged
  keep_md: yes
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path='Figs/',echo = TRUE)
```

## Executive Summary
The goal of the project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants as the data for creating a prediction algorithm. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 
All the information is available from [Data](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

Install the packages silently
```{r, include=FALSE, echo=FALSE}
destination <- getwd()
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
install.packages("data.table")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("lmtest")
install.packages("flextable")
install.packages("caret")
```

Load the libraries
```{r, Load Libraries, echo=FALSE}
library("data.table")
library("ggplot2")
library("lmtest")
library("flextable")
library("dplyr")
library("tinytex")
library("caret")
```

Download all needed data
```{r, Download Data}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","training.csv",method = "curl")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","testing.csv",method = "curl")

training_set <- tibble::as_tibble(fread("training.csv",na.strings=c('#DIV/0!', '', 'NA')))
testing_set  <- tibble::as_tibble(fread("testing.csv",na.strings=c('#DIV/0!', '', 'NA')))
```

# Analyze the Data
Testing Set
```{r, Testing Set, echo=FALSE}
exit_values_1 <- tibble(
  "Observations" = nrow(testing_set),
  "Variables" = ncol(testing_set))
flextable(exit_values_1)
```

Training Set
```{r, Training Set, echo=FALSE}
exit_values_2 <- tibble(
  "Observations" = nrow(training_set),
  "Variables" = ncol(training_set))
flextable(exit_values_2)
```

# Split the Data
```{r, Split the Data}
set.seed(171)
training_sub_set <- createDataPartition( y = training_set$classe,p = 0.7,list = FALSE)
real_training <- training_set[training_sub_set,]
real_validation <- training_set[-training_sub_set,]
```

# Pre-process the data
Remove variables with mostly N/A values
```{r, Pre-process }
NA_vals <- sapply(real_training,function(x) mean(is.na(x))) > 0.95
real_training <- real_training[,NA_vals==FALSE]
real_validation <- real_validation[,NA_vals==FALSE]
```

Remove variables with low variance
```{r, Remove variables}
nzv <- nearZeroVar(real_training) #Using the training across both datasets as there has to be conformity
real_training <- real_training[,-nzv]
real_validation <- real_validation[,-nzv]
```

Remove columns that will have no bearing on the results
(V1, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp)
```{r, Remove columns}
real_training_clean = select(real_training, -V1, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp)
real_validation_clean = select(real_validation, -V1, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp)
```

Testing Set
```{r, Testing Set 2}
exit_values_3 <- tibble(
  "Observations" = nrow(real_validation_clean),
  "Variables" = ncol(real_validation_clean))
flextable(exit_values_3)
```

Training Set
```{r, Training Set 2}
exit_values_4 <- tibble(
  "Observations" = nrow(real_training_clean),
  "Variables" = ncol(real_training_clean))
flextable(exit_values_4)
```

Create validation partitions of sample observations from the Training set
```{r, Create validation}
control <- trainControl(method="cv",number = 10)
```

# Algorithm Performance Check
Gradient Boosting Model
```{r, GBM}
set.seed(171)
model_GBM <- train(classe ~.,
                  data = real_training_clean,
                  method = "gbm",
                  trControl = control,
                  verbose = FALSE)
```

Random Forest
```{r, RF}
set.seed(171)
model_RF  <- train(classe ~.,
                   data = real_training_clean,
                   method = "rf",
                   trControl = control)
```

k-Nearest Neighbors
```{r, KNN}
set.seed(171)
model_KNN <- train(classe~., 
                   data=real_training_clean, 
                   method="knn", 
                   metric="Accuracy", 
                   trControl=control)
```

Linear Discriminant Analysis
```{r, LDA}
set.seed(171)
model_LDA <- train(classe~., 
                   data=real_training_clean, 
                   method="lda", 
                   metric="Accuracy",
                   trControl=control)
```

# Checking performance for all the models
```{r, Checking performance, echo=FALSE}
model_results <- resamples(list(lda=model_LDA, knn=model_KNN, gbm=model_GBM, rf=model_RF))
summary(model_results)
```

# Check performance using training dataset

Gradient Boosting Model
```{r, Check performance GBM}
GBM_Pred <- predict(model_GBM, newdata=real_validation_clean)
conf_Mat_GBM <- confusionMatrix(GBM_Pred, as.factor(real_validation_clean$classe))
print(conf_Mat_GBM)
```

Random Forest
```{r, Check performance RF}
RF_Pred <- predict(model_RF, newdata=real_validation_clean)
conf_Mat_RF <- confusionMatrix(RF_Pred, as.factor(real_validation_clean$classe))
print(conf_Mat_RF)
```

k-Nearest Neighbors
```{r, Check performance KNN}
LDA_Pred <- predict(model_LDA, newdata=real_validation_clean)
conf_Mat_LDA <- confusionMatrix(LDA_Pred, as.factor(real_validation_clean$classe))
print(conf_Mat_LDA)
```

Linear Discriminant Analysis
```{r, Check performance LDA}
KNN_Pred <- predict(model_KNN, newdata=real_validation_clean)
conf_Mat_KNN <- confusionMatrix(KNN_Pred, as.factor(real_validation_clean$classe))
print(conf_Mat_KNN)
```

# Summary of the performance
```{r, Summary, echo=FALSE}
performance <- matrix(round(c(conf_Mat_GBM$overall,conf_Mat_RF$overall,conf_Mat_LDA$overall,conf_Mat_KNN$overall),4), ncol=4)
colnames(performance)<-c('Linear Discrimination Analysis', 'K- Nearest Neighbors','Gradient Boosting','Random Forest')
performance.df <- as.data.frame(performance)
print(qflextable(performance.df))
```

Check the variables with the most influence
```{r, influence}
print(summary(model_GBM))

print(qplot(num_window, roll_belt, data = real_training, col = classe))
print(qplot(pitch_forearm, roll_belt, data = real_training, col = classe))
print(qplot(num_window, pitch_forearm, data = real_training, col = classe))
```

# Prediction on the test dataset
```{r, Prediction}
model_prediction <- predict(model_RF, testing_set)
table(model_prediction,testing_set$problem_id)
```


## Summary
The best model based on the output in the model_results summary is the random forests model with an accuracy of 0.998. Due to the high number of trees, I believe that it is the best model to use.
